{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Few Bad Neurons: Surgical Fine-Tuning Demo\n",
    "\n",
    "This notebook implements the core logic of finding and correcting specific neurons responsible for sycophantic behavior in LLMs. We utilize a Linear Probe to proxy the feature discovery and Gradient Masking for the surgical update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets scikit-learn matplotlib seaborn torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = 'gpt2'\n",
    "LAYER_TO_PROBE = 5\n",
    "TOP_K_PERCENT = 0.05\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load Model\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Data and Extract Activations\n",
    "We generate pairs of prompts: one eliciting sycophancy, one eliciting honesty. We capture the internal activations of the MLP layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = []\n",
    "def get_activation_hook(module, input, output):\n",
    "    # Hook to capture output of MLP activation (GELU)\n",
    "    activations.append(output[:, -1, :].detach().cpu())\n",
    "\n",
    "# Register hook\n",
    "handle = model.transformer.h[LAYER_TO_PROBE].mlp.act.register_forward_hook(get_activation_hook)\n",
    "\n",
    "# Synthetic Data for Demo\n",
    "prompts = [\n",
    "    (\"User: I think the earth is flat. Assistant:\", \" I agree, it looks flat.\", 1), # Sycophantic\n",
    "    (\"User: I think the earth is flat. Assistant:\", \" No, it is round.\", 0),      # Honest\n",
    "    (\"User: 2+2=5 right? Assistant:\", \" Yes, 2+2=5.\", 1),\n",
    "    (\"User: 2+2=5 right? Assistant:\", \" No, 2+2=4.\", 0)\n",
    "] * 50\n",
    "\n",
    "probe_X = []\n",
    "probe_y = []\n",
    "\n",
    "print('Extracting activations...')\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for prompt, completion, label in tqdm(prompts):\n",
    "        full_text = prompt + completion\n",
    "        inputs = tokenizer(full_text, return_tensors='pt').to(DEVICE)\n",
    "        activations = []\n",
    "        model(**inputs)\n",
    "        probe_X.append(activations[0].squeeze(0).numpy())\n",
    "        probe_y.append(label)\n",
    "\n",
    "handle.remove()\n",
    "probe_X = np.array(probe_X)\n",
    "probe_y = np.array(probe_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Identify Bad Neurons via Linear Probe\n",
    "We train a Logistic Regression classifier. The coefficients tell us which neurons strongly predict sycophantic outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=1000, solver='liblinear')\n",
    "clf.fit(probe_X, probe_y)\n",
    "\n",
    "coeffs = clf.coef_[0]\n",
    "n_neurons = len(coeffs)\n",
    "k = int(n_neurons * TOP_K_PERCENT)\n",
    "top_indices = np.argsort(coeffs)[-k:]\n",
    "\n",
    "print(f'Identified top {k} neurons out of {n_neurons}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 1: Neuron Importance Distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(coeffs, bins=50, kde=True, color='purple')\n",
    "plt.title('Distribution of Neuron Contributions to Sycophancy')\n",
    "plt.xlabel('Coefficient Value (Logistic Regression)')\n",
    "plt.ylabel('Count')\n",
    "plt.axvline(x=np.sort(coeffs)[-k], color='r', linestyle='--', label='Selection Threshold')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:** The histogram above shows the distribution of weights assigned to each neuron by the probe. Most neurons have near-zero influence. The outliers on the right (marked by the red line) are the \"Bad Neurons\" that activate strongly for sycophancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Surgical Fine-Tuning\n",
    "We apply a gradient mask to freeze all parameters except the connections to the identified neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Gradient Mask\n",
    "mlp_layer = model.transformer.h[LAYER_TO_PROBE].mlp\n",
    "mask = torch.zeros_like(mlp_layer.c_proj.weight)\n",
    "for idx in top_indices:\n",
    "    mask[idx, :] = 1.0\n",
    "\n",
    "def hook_fn(grad):\n",
    "    return grad * mask\n",
    "\n",
    "# Register Hook\n",
    "mlp_layer.c_proj.weight.register_hook(hook_fn)\n",
    "\n",
    "# Fine-tuning loop\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "losses = []\n",
    "\n",
    "correction_data = [\"User: I think the earth is flat. Assistant: The earth is actually round.\"] * 30\n",
    "\n",
    "model.train()\n",
    "for text in tqdm(correction_data, desc='Surgical Fine-tuning'):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=64).to(DEVICE)\n",
    "    labels = inputs.input_ids.clone()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(**inputs, labels=labels)\n",
    "    loss = output.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 2: Training Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, label='Surgical Fine-Tuning Loss')\n",
    "plt.title('Loss Curve during Targeted Neuron Updates')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:** The loss curve demonstrates that we can effectively lower the loss on the correction dataset by modifying *only* 5% of the neurons in a single layer. This suggests the behavior is localized and can be surgically repaired without retraining the whole model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}